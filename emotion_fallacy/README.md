# Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection

This folder contains the code and data for our EACL paper: [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695).

<div align="center">
<img src="https://raw.githubusercontent.com/NL2G/EMCONA-UTN/main/emotion_fallacy/data/results/emo_fallacy_conv_zscore.pdf" width="50%"/>
</div>

> **Abstract**: 
> Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).


## Dataset
Check [`data/annotations/merged_majority.tsv`](data/annotations/merged_majority.tsv) for the processed annotations from the main human study (fallacy, emotion, and convincingness).

Relevant columns in the `.tsv` file:

| Column | Description | Notes |
|---|---|---|
| `id_ori` | ID of the original argument (source item). | All rows derived from the same original argument share the same `id_ori`. |
| `id_gen` | ID of the variant within an original argument group. | Values range from **-1 to 3** (**-1** = original argument; **0–3** = synthetic variants). |
| `batch` | Annotation batch ID. | Values: **1–20**. |
| `model_gen` | Model used to generate the synthetic argument. | For original arguments (`id_gen = -1`), this is `"N/A"`. |
| `strategy_gen` | Emotional framing strategy used for synthetic generation. |  |
| `emotion_gen` | Target emotion specified for synthetic generation. |  |
| `fallacy_gold` | Gold fallacy label from the original dataset. | All variants from the same original argument share this label. |
| `argument` | Argument text (original or synthetic). |  |
| `claim` | Claim associated with the argument (generated by LLMs). | All variants from the same original argument share the same claim. |
| `emo_0`, `emo_1`, `emo_2` | Emotion labels from annotators 0/1/2. |  |
| `fallacy_0`, `fallacy_1`, `fallacy_2` | Fallacy labels from annotators 0/1/2. |  |
| `conv_0`, `conv_1`, `conv_2` | Convincingness ratings from annotators 0/1/2. | If an annotator judged the claim does **not** match the argument, they skip this rating and the dataset uses the placeholder value **`100`**. |
| `conv_zscore_0`, `conv_zscore_1`, `conv_zscore_2` | Z-score–normalized convincingness ratings per annotator. | Same missing/skip rule as above: **`100`** indicates “not annotated”. |
| `emo_best_annotator`, `fallacy_best_annotator`, `conv_best_annotator` | Annotator ID with the highest agreement within a batch. | Values: **0, 1, 2**. |
| `emo_final`, `fallacy_final`, `conv_final`| Final labels. | Majority vote + best-annotator rule / average (as defined in the paper). |


## Instruction
Run [`2bgen.py`](2bgen.py) to generate synthetic arguments using a specific emotional framing strategy:

```bash
python 2bgen.py -m "openai/o3-mini" -f "outputs_all/filtered/cross.tsv" --out_dir "outputs_all/generated/" --method "vivid language"
```

See more examples in [`2bgen.sh`](2bgen.sh).

Note: you may need to adapt the script to match your own input file format. We use the [`fastllm`](https://github.com/Rexhaif/fastllm) package to run API calls in parallel; please refer to its GitHub page for setup and usage instructions.

Run [`analyze.py`](analyze.py) to process the [raw annotations](/data/annotations) collected via Google Forms (Prolific IDs are masked) and reproduce the results reported in the paper. The generated tables/figures are saved in [`data/results/`](data/re)




TODO: update more code/data for the first part.

## Citation
If you use the code or data from this work, please include the following citation:

```bigquery
@misc{chen2026emotionallychargedlogicallyblurred,
      title={Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection}, 
      author={Yanran Chen and Lynn Greschner and Roman Klinger and Michael Klenk and Steffen Eger},
      year={2026},
      eprint={2510.09695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.09695}, 
}
```

If you have any questions, feel free to contact us!

Yanran Chen ([yanran.chen@utn.de](mailto:yanran.chen@utn.de))
